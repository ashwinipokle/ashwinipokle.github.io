---
layout: post
title: "Consistency Models Made Easy"
date:  2025-05-06 00:00:59 +00:00
image: /images/cost-ect.jpg
categories: research
authors: "Zhengyang Geng, <strong>Ashwini Pokle</strong>, William Luo, Justin Lin, and Zico Kolter"
venue: "ICLR"
arxiv: https://arxiv.org/abs/2406.14548
code: https://github.com/locuslab/ect
---
We propose an alternative scheme for training Consistency Models (CM), vastly improving the efficiency of building such models. We  express CM trajectories via a particular differential equation and argue that diffusion models can be viewed as a special case of CMs with a specific discretization. We can thus fine-tune a consistency model starting from a pre-trained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly improved training times while indeed improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained of hundreds of GPU hours.
